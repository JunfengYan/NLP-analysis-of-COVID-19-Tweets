{"cells":[{"cell_type":"markdown","source":["##Setting up environment and importing packages"],"metadata":{}},{"cell_type":"code","source":["import os\nos.environ[\"PYSPARK_PYTHON\"] = \"python3\""],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession \\\n    .builder \\\n    .appName(\"moive analysis\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nimport math\n\n\n%matplotlib inline"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["datapath = \"dbfs:/FileStore/tables/covid_train_hydrated.csv\""],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["## reading and cleaning the data"],"metadata":{}},{"cell_type":"code","source":["tweets_Df =  spark.read.format(\"csv\").option(\"header\", \"true\").load(datapath)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["#dropping null rows\nfrom pyspark.sql import functions as F\n\ntweets = tweets_Df.select(\"id\",\"text\")\n\ndef pre_process(df):\n  df_drop = df.filter(df['text'].isNotNull())\n  df_drop = df_drop.filter(df_drop['id'].isNotNull())\n  df_drop = df_drop.dropDuplicates()\n  \n  print('After dropping, we have ', str(df_drop.count()), 'row in dataframe')\n  return df_drop\n\ntweets_drop = pre_process(tweets)\n\n\n"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">After dropping, we have  909597 row in dataframe\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["#dropping text rows that has \"false\" as value\ntweets_drop = tweets_drop.where(~tweets_drop.text.like('false'))\ntweets_drop.count()"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[7]: 901642</div>"]}}],"execution_count":9},{"cell_type":"code","source":["#used regular expression matching to get rid of urls\ntweets_clean = tweets_drop.select('id', (F.lower(F.regexp_replace('text', \"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \"\")).alias('text')))\n"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["#total number of tweets left\ntweets_clean.count()"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[36]: 901642</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["##Building the pre-processing pipeline"],"metadata":{}},{"cell_type":"code","source":["#spark nlp does not comes with collection of stop words\n#so I used the English stop words from the nltk library\nnltk.download('stopwords')\n\nstopwords = nltk.corpus.stopwords.words('english')\nstopwords.append(\"amp\")\n\nprint (\"We use \" + str(len(stopwords)) + \" stop-words from nltk library.\")\nprint (stopwords[:10])"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\nWe use 180 stop-words from nltk library.\n[&#39;i&#39;, &#39;me&#39;, &#39;my&#39;, &#39;myself&#39;, &#39;we&#39;, &#39;our&#39;, &#39;ours&#39;, &#39;ourselves&#39;, &#39;you&#39;, &#34;you&#39;re&#34;]\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["#building text pre processing pipeline with Spark NLP and Spark ML\n\nimport sparknlp\n\nfrom sparknlp.annotator import *\nfrom sparknlp.common import *\nfrom sparknlp.base import *\n\nfrom pyspark.ml import Pipeline\n\n#document assembler transform raw text into annotator type, common object type used in spark nlp anootator interface\n\ndocument_assembler = DocumentAssembler() \\\n    .setInputCol(\"text\") \\\n    .setOutputCol(\"document\")\n    \nsentence_detector = SentenceDetector() \\\n    .setInputCols([\"document\"]) \\\n    .setOutputCol(\"sentence\") \\\n    .setUseAbbreviations(True)\n\n#tokenizer transform sentences into individual words for each tweet\n\ntokenizer = Tokenizer() \\\n    .setInputCols(['sentence']) \\\n    .setSplitPattern(\"\\p{P}(?<!')\") \\\n    .setMinLength(3)\\\n    .setMaxLength(16)\\\n    .setOutputCol(\"token\")\n\n#Removes all dirty characters from text following a regex pattern and transforms words based on a provided dictionary\n#this step is probably redundant here since we already did the text cleaning above\n\nnormalizer = Normalizer() \\\n    .setInputCols([\"token\"]) \\\n    .setOutputCol(\"normalized\")\\\n    .setLowercase(True)\\\n    .setCleanupPatterns([\"[^\\w\\d\\s]\"])\n\n#drop all stop words i.e. words with no inherent meaning in the list of strings\nstopwords_cleaner = StopWordsCleaner() \\\n        .setInputCols([\"normalized\"]) \\\n        .setStopWords(stopwords)\\\n        .setCaseSensitive(False)\\\n        .setOutputCol(\"removed\")\n\n#retrieving the meaningful part of the word i.e. stem without prefix or suffixes\n#stemmer = Stemmer() \\\n    #.setInputCols([\"removed\"]) \\\n    #.setOutputCol(\"stem\")\n\n# using a dictionary or through morphological analysis to return words to their base form\nlemmatizer = LemmatizerModel.pretrained() \\\n     .setInputCols(['removed']) \\\n     .setOutputCol('lemma')\n\n#generate bi-grams and tri-grams from the lemmatized tokens\nngrams_cum = NGramGenerator() \\\n            .setInputCols([\"lemma\"]) \\\n            .setOutputCol(\"ngrams\") \\\n            .setN(3) \\\n            .setEnableCumulative(True)\n#produced processed text data\nfinisher = Finisher() \\\n    .setInputCols([\"removed\",\"lemma\",\"ngrams\"]) \\\n    .setOutputAsArray(True) \\\n    .setCleanAnnotations(True)\n\n#assembling the pipeline\nnlpPipeline = Pipeline(stages=[document_assembler,sentence_detector,tokenizer,normalizer,stopwords_cleaner,lemmatizer,ngrams_cum,finisher])                    "],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">lemma_antbnc download started this may take some time.\nApproximate size to download 907.6 KB\n\r[ | ]\r[ / ]\r[OK!]\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["#fitting the model and transforming the data\nmod = nlpPipeline.fit(tweets_clean)\ncleaned = mod.transform(tweets_clean)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":["cleaned.select('finished_lemma','finished_ngrams').show(10,truncate = False)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nfinished_lemma                                                                                                  |finished_ngrams                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n+----------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n[anyone, explain, people, think, azithromycin, treat, coronavirus]                                              |[anyone, explain, people, think, anyone explain, explain people, people think, anyone explain people, explain people think, azithromycin, treat, coronavirus, azithromycin treat, treat coronavirus, azithromycin treat coronavirus]                                                                                                                                                                                                                                                                                                                                     |\n[govparsonmo, please, listen, many, voice, across, state, request, plead, take, immediate, preventative, action]|[govparsonmo, please, listen, many, voice, across, state, request, plead, take, immediate, preventative, action, govparsonmo please, please listen, listen many, many voice, voice across, across state, state request, request plead, plead take, take immediate, immediate preventative, preventative action, govparsonmo please listen, please listen many, listen many voice, many voice across, voice across state, across state request, state request plead, request plead take, plead take immediate, take immediate preventative, immediate preventative action]|\n[always, look, special, interest, lose, lot, kickback, potus, helm]                                             |[always, look, special, interest, always look, look special, special interest, always look special, look special interest, lose, lot, kickback, potus, helm, lose lot, lot kickback, kickback potus, potus helm, lose lot kickback, lot kickback potus, kickback potus helm]                                                                                                                                                                                                                                                                                             |\n[nice, thing, lol]                                                                                              |[nice, thing, lol, nice thing, thing lol, nice thing lol]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n[vibe, need]                                                                                                    |[vibe, need, vibe need]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n[around, somebody, call, people, cuz, snitch]                                                                   |[around, somebody, call, people, cuz, snitch, around somebody, somebody call, call people, people cuz, cuz snitch, around somebody call, somebody call people, call people cuz, people cuz snitch]                                                                                                                                                                                                                                                                                                                                                                       |\n[]                                                                                                              |[]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n[terrible, new, rip]                                                                                            |[terrible, new, terrible new, rip]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n[welcome, encourage, insight, present, homelearning, onlineschoolong, sirkenrobinson]                           |[welcome, encourage, insight, present, homelearning, encourage insight, insight present, present homelearning, encourage insight present, insight present homelearning, onlineschoolong, sirkenrobinson, onlineschoolong sirkenrobinson]                                                                                                                                                                                                                                                                                                                                 |\n[peter, frampton, feel]                                                                                         |[peter, frampton, feel, peter frampton, frampton feel, peter frampton feel]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n+----------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["#using regex matching to unify different namings of COVID-19\ntraining = cleaned.select(\"id\",F.explode(\"finished_ngrams\").alias(\"elements\"))\\\n          .select(\"id\",F.regexp_replace(\"elements\",'(\\w*corona\\w*)|(\\w*covid\\w*)',\"coronavirus\").alias(\"elements\"))\\\n          .groupby(\"id\")\\\n          .agg(F.collect_list(\"elements\").alias(\"ngrams\")) \n        "],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"code","source":["training.select(\"ngrams\").show(5,truncate = False)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nngrams                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n[marshablackburn, also, coronavirus, hoax, remember, marshablackburn also, also coronavirus, coronavirus hoax, hoax remember, marshablackburn also coronavirus, also coronavirus hoax, coronavirus hoax remember]                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n[government, overreact, government overreact, coronavirus, thelouvre, coronavirus thelouvre]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n[confirm, case, coronavirus, south, america, confirm case, case coronavirus, coronavirus south, south america, confirm case coronavirus, case coronavirus south, coronavirus south america, argentina, chile, ecuador, brazil, argentina chile, chile ecuador, ecuador brazil, argentina chile ecuador, chile ecuador brazil, country, temperature, warm, virus, active, spread, country temperature, temperature warm, warm virus, virus active, active spread, country temperature warm, temperature warm virus, warm virus active, virus active spread, 158, confirm, case, 158 confirm, confirm case, 158 confirm case, realdonaldtrump, gop, silent, gop silent, stupid]|\n[live, nation, reportedly, suspend, tour, amid, coronavirus, pandemic, live nation, nation reportedly, reportedly suspend, suspend tour, tour amid, amid coronavirus, coronavirus pandemic, live nation reportedly, nation reportedly suspend, reportedly suspend tour, suspend tour amid, tour amid coronavirus, amid coronavirus pandemic]                                                                                                                                                                                                                                                                                                                                 |\n[help, italy, help italy, great, trumpland, protect, people, china, help, italy, great trumpland, trumpland protect, protect people, people china, china help, help italy, great trumpland protect, trumpland protect people, protect people china, people china help, china help italy]                                                                                                                                                                                                                                                                                                                                                                                     |\n+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["from pyspark.ml.feature import CountVectorizer, IDF\n\n#obtaining tf-idf matrix\nTF = CountVectorizer(inputCol=\"ngrams\", outputCol=\"rawFeatures\",minDF=0.01,maxDF=0.99,vocabSize = 1000)\ntf_model = TF.fit(training)\nfeaturizedData = tf_model.transform(training)\n# alternatively, CountVectorizer can also be used to get term frequency vectors\n\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\nidfModel = idf.fit(featurizedData)\nrescaledData = idfModel.transform(featurizedData)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":19},{"cell_type":"code","source":["#our bag of words\nvocab = tf_model.vocabulary\nvocab"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[14]: [&#39;coronavirus&#39;,\n &#39;get&#39;,\n &#39;people&#39;,\n &#39;trump&#39;,\n &#39;test&#39;,\n &#39;say&#39;,\n &#39;like&#39;,\n &#39;need&#39;,\n &#39;one&#39;,\n &#39;mask&#39;,\n &#39;make&#39;,\n &#39;time&#39;,\n &#39;case&#39;,\n &#39;go&#39;,\n &#39;know&#39;,\n &#39;new&#39;,\n &#39;pandemic&#39;,\n &#39;day&#39;,\n &#39;take&#39;,\n &#39;good&#39;,\n &#39;see&#39;,\n &#39;death&#39;,\n &#39;work&#39;,\n &#39;think&#39;,\n &#39;realdonaldtrump&#39;,\n &#39;state&#39;,\n &#39;well&#39;,\n &#39;would&#39;,\n &#39;thank&#39;,\n &#39;die&#39;,\n &#39;virus&#39;,\n &#39;help&#39;,\n &#39;home&#39;,\n &#39;want&#39;,\n &#39;come&#39;,\n &#39;back&#39;,\n &#39;right&#39;,\n &#39;coronavirus coronavirus&#39;,\n &#39;still&#39;,\n &#39;today&#39;,\n &#39;american&#39;,\n &#39;life&#39;,\n &#39;wear&#39;,\n &#39;please&#39;,\n &#39;via&#39;,\n &#39;let&#39;,\n &#39;many&#39;,\n &#39;look&#39;,\n &#39;great&#39;,\n &#39;year&#39;,\n &#39;health&#39;,\n &#39;week&#39;,\n &#39;news&#39;,\n &#39;thing&#39;,\n &#39;keep&#39;,\n &#39;stay&#39;,\n &#39;even&#39;,\n &#39;school&#39;,\n &#39;way&#39;,\n &#39;care&#39;,\n &#39;give&#39;,\n &#39;country&#39;,\n &#39;really&#39;,\n &#39;number&#39;,\n &#39;coronavirus pandemic&#39;,\n &#39;tell&#39;,\n &#39;open&#39;,\n &#39;call&#39;,\n &#39;stop&#39;,\n &#39;could&#39;,\n &#39;first&#39;,\n &#39;bad&#39;,\n &#39;positive&#39;,\n &#39;family&#39;,\n &#39;president&#39;,\n &#39;use&#39;,\n &#39;world&#39;,\n &#39;spread&#39;,\n &#39;coronavirus case&#39;,\n &#39;000&#39;,\n &#39;love&#39;,\n &#39;report&#39;,\n &#39;wear mask&#39;,\n &#39;live&#39;,\n &#39;much&#39;,\n &#39;show&#39;,\n &#39;month&#39;,\n &#39;start&#39;,\n &#39;due&#39;,\n &#39;try&#39;,\n &#39;dont&#39;,\n &#39;america&#39;,\n &#39;everyone&#39;,\n &#39;every&#39;,\n &#39;may&#39;,\n &#39;also&#39;,\n &#39;read&#39;,\n &#39;county&#39;,\n &#39;kill&#39;,\n &#39;business&#39;,\n &#39;happen&#39;,\n &#39;safe&#39;,\n &#39;lie&#39;,\n &#39;close&#39;,\n &#39;hope&#39;,\n &#39;man&#39;,\n &#39;never&#39;,\n &#39;job&#39;,\n &#39;last&#39;,\n &#39;since&#39;,\n &#39;2020&#39;,\n &#39;put&#39;,\n &#39;watch&#39;,\n &#39;another&#39;,\n &#39;support&#39;,\n &#39;feel&#39;,\n &#39;update&#39;,\n &#39;face&#39;,\n &#39;vote&#39;,\n &#39;shit&#39;,\n &#39;florida&#39;,\n &#39;coronavirus test&#39;,\n &#39;long&#39;,\n &#39;vaccine&#39;,\n &#39;high&#39;,\n &#39;social&#39;,\n &#39;find&#39;,\n &#39;public&#39;,\n &#39;big&#39;,\n &#39;community&#39;,\n &#39;real&#39;,\n &#39;patient&#39;,\n &#39;hospital&#39;,\n &#39;talk&#39;,\n &#39;friend&#39;,\n &#39;house&#39;,\n &#39;plan&#39;,\n &#39;lose&#39;,\n &#39;hear&#39;,\n &#39;response&#39;,\n &#39;governor&#39;,\n &#39;believe&#39;]</div>"]}}],"execution_count":20},{"cell_type":"code","source":["len(vocab)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[48]: 142</div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["## trainning k-means ++ model"],"metadata":{}},{"cell_type":"code","source":["#training with all 142 vocabulary\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\n\nevaluator = ClusteringEvaluator()\n#spark ml's version of KMeans choose K-means ++ as the initialization algorithm by default\n#Optimize choice of k\n#for each k, use a different 10% sample to fit and compute silhouette distance\ncost = np.zeros(20)\nfor k in range(2,20):\n    kmeans = KMeans().setK(k).setSeed(565).setFeaturesCol(\"features\").setInitSteps(10)\n    model = kmeans.fit(rescaledData.sample(False,0.1, seed=42))\n    pred = model.transform(rescaledData)\n    cost[k] = evaluator.evaluate(pred)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"code","source":["plt.figure(dpi=1200)\nfig, ax = plt.subplots(1,1, figsize =(8,6))\nax.plot(range(2,20),cost[2:20], linestyle='--', marker='o')\nax.set_xlabel('k')\nax.set_ylabel('cost')\nax.set_title('Silhouette for k from 2 to 20')\nfor x,y in zip([6,7,9,12],[cost[6],cost[7],cost[9],cost[12]]):\n  xlab = \"{:.2f}\".format(x)\n  plt.annotate((\"K = \" + xlab),(x,y),textcoords=\"offset points\",xytext = (0,5),ha = 'center')\nplt.savefig('this.png')\ndisplay()"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"image/png":"/plots/f14dc6cd-20a0-476b-a329-736cf10f8254.png"}}],"execution_count":24},{"cell_type":"code","source":["#fitting the K-means with the optimal K\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\n\nbest_K_means = KMeans().setK(7).setSeed(565).setFeaturesCol(\"features\").setInitSteps(10)\nbest_model = best_K_means.fit(rescaledData)\npredictions = best_model.transform(rescaledData)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":25},{"cell_type":"code","source":["best_model.summary.clusterSizes"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[30]: [581559, 116838, 53341, 139137]</div>"]}}],"execution_count":26},{"cell_type":"code","source":["centers = best_model.clusterCenters()"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":27},{"cell_type":"code","source":["def printCenters1(centers):\n  for i in range(len(centers)):\n    print (\"Cluster \" + str(i) + \" words:\", end='')\n    order_centroids = centers[i].argsort()[::-1]\n    for ind in order_centroids[:10]:\n        print(vocab[ind] + \",\",end = '')\n    print()"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":28},{"cell_type":"code","source":["len(centers)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[92]: 7</div>"]}}],"execution_count":29},{"cell_type":"code","source":["printCenters1(centers)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Cluster 0 words:coronavirus,trump,get,say,need,make,realdonaldtrump,well,go,know,\nCluster 1 words:good,like,thank,pandemic,time,coronavirus,via,coronavirus pandemic,coronavirus coronavirus,look,\nCluster 2 words:test,case,coronavirus case,positive,coronavirus test,coronavirus,new,report,death,get,\nCluster 3 words:people,get,coronavirus,mask,wear,trump,say,wear mask,one,die,\n</div>"]}}],"execution_count":30},{"cell_type":"code","source":["def getCenters1(centers):\n  centers_summary = {}\n  for i in range(len(centers)):\n    summary = []\n    order_centroids = centers[i].argsort()[::-1]\n    for ind in order_centroids[:20]:\n       summary.append(vocab[ind])\n    centers_summary[i] = summary\n  return centers_summary"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":31},{"cell_type":"code","source":["summary = getCenters1(centers)\n"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[97]: {0: [&#39;happen&#39;,\n  &#39;coronavirus&#39;,\n  &#39;would&#39;,\n  &#39;go&#39;,\n  &#39;people&#39;,\n  &#39;thing&#39;,\n  &#39;get&#39;,\n  &#39;see&#39;,\n  &#39;know&#39;,\n  &#39;like&#39;,\n  &#39;make&#39;,\n  &#39;think&#39;,\n  &#39;still&#39;,\n  &#39;trump&#39;,\n  &#39;need&#39;,\n  &#39;say&#39;,\n  &#39;let&#39;,\n  &#39;never&#39;,\n  &#39;time&#39;,\n  &#39;test&#39;],\n 1: [&#39;coronavirus&#39;,\n  &#39;get&#39;,\n  &#39;people&#39;,\n  &#39;trump&#39;,\n  &#39;test&#39;,\n  &#39;say&#39;,\n  &#39;like&#39;,\n  &#39;need&#39;,\n  &#39;time&#39;,\n  &#39;make&#39;,\n  &#39;know&#39;,\n  &#39;go&#39;,\n  &#39;pandemic&#39;,\n  &#39;take&#39;,\n  &#39;good&#39;,\n  &#39;realdonaldtrump&#39;,\n  &#39;think&#39;,\n  &#39;work&#39;,\n  &#39;see&#39;,\n  &#39;well&#39;],\n 2: [&#39;case&#39;,\n  &#39;coronavirus case&#39;,\n  &#39;new&#39;,\n  &#39;coronavirus&#39;,\n  &#39;death&#39;,\n  &#39;report&#39;,\n  &#39;county&#39;,\n  &#39;state&#39;,\n  &#39;number&#39;,\n  &#39;day&#39;,\n  &#39;000&#39;,\n  &#39;test&#39;,\n  &#39;positive&#39;,\n  &#39;florida&#39;,\n  &#39;high&#39;,\n  &#39;today&#39;,\n  &#39;week&#39;,\n  &#39;say&#39;,\n  &#39;people&#39;,\n  &#39;see&#39;],\n 3: [&#39;business&#39;,\n  &#39;coronavirus&#39;,\n  &#39;open&#39;,\n  &#39;help&#39;,\n  &#39;close&#39;,\n  &#39;get&#39;,\n  &#39;support&#39;,\n  &#39;people&#39;,\n  &#39;need&#39;,\n  &#39;work&#39;,\n  &#39;pandemic&#39;,\n  &#39;make&#39;,\n  &#39;state&#39;,\n  &#39;back&#39;,\n  &#39;go&#39;,\n  &#39;take&#39;,\n  &#39;time&#39;,\n  &#39;coronavirus pandemic&#39;,\n  &#39;many&#39;,\n  &#39;like&#39;],\n 4: [&#39;great&#39;,\n  &#39;coronavirus&#39;,\n  &#39;job&#39;,\n  &#39;thank&#39;,\n  &#39;work&#39;,\n  &#39;news&#39;,\n  &#39;time&#39;,\n  &#39;make&#39;,\n  &#39;get&#39;,\n  &#39;see&#39;,\n  &#39;people&#39;,\n  &#39;trump&#39;,\n  &#39;good&#39;,\n  &#39;america&#39;,\n  &#39;day&#39;,\n  &#39;need&#39;,\n  &#39;like&#39;,\n  &#39;pandemic&#39;,\n  &#39;help&#39;,\n  &#39;one&#39;],\n 5: [&#39;one&#39;,\n  &#39;coronavirus&#39;,\n  &#39;get&#39;,\n  &#39;people&#39;,\n  &#39;love&#39;,\n  &#39;day&#39;,\n  &#39;say&#39;,\n  &#39;thing&#39;,\n  &#39;time&#39;,\n  &#39;know&#39;,\n  &#39;like&#39;,\n  &#39;test&#39;,\n  &#39;trump&#39;,\n  &#39;good&#39;,\n  &#39;take&#39;,\n  &#39;see&#39;,\n  &#39;make&#39;,\n  &#39;go&#39;,\n  &#39;need&#39;,\n  &#39;die&#39;],\n 6: [&#39;mask&#39;,\n  &#39;wear&#39;,\n  &#39;wear mask&#39;,\n  &#39;people&#39;,\n  &#39;coronavirus&#39;,\n  &#39;social&#39;,\n  &#39;face&#39;,\n  &#39;get&#39;,\n  &#39;please&#39;,\n  &#39;stay&#39;,\n  &#39;one&#39;,\n  &#39;public&#39;,\n  &#39;spread&#39;,\n  &#39;say&#39;,\n  &#39;like&#39;,\n  &#39;make&#39;,\n  &#39;see&#39;,\n  &#39;go&#39;,\n  &#39;home&#39;,\n  &#39;everyone&#39;]}</div>"]}}],"execution_count":32},{"cell_type":"code","source":["def findIdenticalWords(summary):\n  out = []\n  for i in range(len(summary)):\n    xx = set(summary[i])\n    out.append(xx)\n  u = set.intersection(*out)\n  \n  return u\n  \nu = findIdenticalWords(summary)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":33},{"cell_type":"code","source":["u"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[106]: {&#39;coronavirus&#39;, &#39;people&#39;}</div>"]}}],"execution_count":34},{"cell_type":"markdown","source":["using lower dimensionality to see if we have better results?"],"metadata":{}},{"cell_type":"code","source":["#limit the dictionary to only top 30 \nTF_small = CountVectorizer(inputCol=\"ngrams\", outputCol=\"rawFeatures\",minDF=0.01,maxDF=0.99,vocabSize = 30)\ntf_model_small = TF_small.fit(training)\nsmall_features = tf_model_small.transform(training)\n# alternatively, CountVectorizer can also be used to get term frequency vectors\n\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\nsmallIdf = idf.fit(small_features)\nsmall_data = smallIdf.transform(small_features)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["tf_model_small.vocabulary"],"metadata":{},"outputs":[],"execution_count":37}],"metadata":{"name":"covidTweetsCluster","notebookId":3361614826123071},"nbformat":4,"nbformat_minor":0}